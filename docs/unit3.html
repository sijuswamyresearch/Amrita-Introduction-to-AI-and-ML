<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Unit 3: Tools, Processes, and Applications in AI and DS</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-dfb324f25d9b1687192fa8be62ac8f9c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="logo.png" class="img-fluid figure-img"></p>
<figcaption>Introduction to AI &amp; ML</figcaption>
</figure>
</div>
</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#basic-tools-for-ai-and-ds" id="toc-basic-tools-for-ai-and-ds" class="nav-link" data-scroll-target="#basic-tools-for-ai-and-ds">Basic Tools for AI and DS</a>
  <ul class="collapse">
  <li><a href="#python-the-language-for-ai-ds" id="toc-python-the-language-for-ai-ds" class="nav-link" data-scroll-target="#python-the-language-for-ai-ds"><code>Python</code>: The language for AI &amp; DS</a></li>
  <li><a href="#key-python-libraries-for-ai-and-ds" id="toc-key-python-libraries-for-ai-and-ds" class="nav-link" data-scroll-target="#key-python-libraries-for-ai-and-ds">Key <code>Python</code> libraries for AI and DS</a></li>
  </ul></li>
  <li><a href="#introduction-to-ds-process-pipeline" id="toc-introduction-to-ds-process-pipeline" class="nav-link" data-scroll-target="#introduction-to-ds-process-pipeline">Introduction to DS Process Pipeline</a>
  <ul class="collapse">
  <li><a href="#business-understanding-or-problem-definition" id="toc-business-understanding-or-problem-definition" class="nav-link" data-scroll-target="#business-understanding-or-problem-definition">Business Understanding or Problem Definition</a></li>
  <li><a href="#data-acquisition" id="toc-data-acquisition" class="nav-link" data-scroll-target="#data-acquisition">Data Acquisition</a></li>
  <li><a href="#eda" id="toc-eda" class="nav-link" data-scroll-target="#eda">EDA</a></li>
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">Data Preparation</a></li>
  <li><a href="#modeling" id="toc-modeling" class="nav-link" data-scroll-target="#modeling">Modeling</a></li>
  <li><a href="#evaluation" id="toc-evaluation" class="nav-link" data-scroll-target="#evaluation">Evaluation</a></li>
  <li><a href="#deployment" id="toc-deployment" class="nav-link" data-scroll-target="#deployment">Deployment</a></li>
  <li><a href="#monitoring-and-maintenance" id="toc-monitoring-and-maintenance" class="nav-link" data-scroll-target="#monitoring-and-maintenance">Monitoring and Maintenance</a></li>
  </ul></li>
  <li><a href="#different-representations-of-data" id="toc-different-representations-of-data" class="nav-link" data-scroll-target="#different-representations-of-data">Different Representations of Data</a>
  <ul class="collapse">
  <li><a href="#importance-of-pre-processing-the-data" id="toc-importance-of-pre-processing-the-data" class="nav-link" data-scroll-target="#importance-of-pre-processing-the-data">Importance of pre-processing the data</a></li>
  </ul></li>
  <li><a href="#elementary-applications-of-ai-and-ds" id="toc-elementary-applications-of-ai-and-ds" class="nav-link" data-scroll-target="#elementary-applications-of-ai-and-ds">Elementary Applications of AI and DS</a>
  <ul class="collapse">
  <li><a href="#classification-tasks" id="toc-classification-tasks" class="nav-link" data-scroll-target="#classification-tasks">Classification tasks</a></li>
  <li><a href="#regression-tasks" id="toc-regression-tasks" class="nav-link" data-scroll-target="#regression-tasks">Regression tasks</a></li>
  <li><a href="#clustering-tasks" id="toc-clustering-tasks" class="nav-link" data-scroll-target="#clustering-tasks">Clustering tasks</a></li>
  <li><a href="#simple-recommendation-systems" id="toc-simple-recommendation-systems" class="nav-link" data-scroll-target="#simple-recommendation-systems">Simple recommendation systems</a></li>
  </ul></li>
  <li><a href="#unit-review" id="toc-unit-review" class="nav-link" data-scroll-target="#unit-review">Unit review</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="unit3.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Unit 3: Tools, Processes, and Applications in AI and DS</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In our discussions so far, we have explored the fundamental ideas behind Artificial Intelligence, the concept of intelligent agents that perceive and act in environments, and the essential statistical tools used to describe data. Now, in this unit, we transition from these foundational concepts to the more tangible aspects of <em>doing</em> AI and DS. We will focus on the practical tools commonly used by professionals, the structured processes that guide data-driven projects, and some initial examples of how these powerful techniques are applied to solve real-world challenges.</p>
<p>Our exploration will begin with an introduction to some <em>basic yet powerful tools</em>, primarily centering on the <code>Python</code> programming language and its rich ecosystem of specialized libraries which have become indispensable in the AI and DS landscape. Following this, we will undertake a guided tour of the <em>DS process pipeline</em>. This pipeline offers a systematic framework for approaching and solving problems using data, taking us from the initial understanding of a problem all the way through to deploying a solution. A critical aspect of working with data is understanding its various <em>representations</em>, as data can come in many forms, each requiring different handling. Equally crucial is the stage of <em>data pre-processing</em>. Raw data, as collected from the real world, is rarely perfect; it often needs to be cleaned, transformed, and prepared before it can be effectively used for analysis or to train intelligent models. Finally, we will touch upon some <em>elementary applications of AI and DS</em>. These examples will provide a glimpse into the practical power of these fields and serve as a bridge to more advanced topics you might encounter in your future studies.</p>
<p>By the conclusion of this unit, you should have a good familiarity with common software tools used in the field, a clear understanding of the typical workflow involved in a DS project, a strong appreciation for why data quality and preparation are foremost important, and a recognition of some basic ways AI and DS are applied to create value.</p>
</section>
<section id="basic-tools-for-ai-and-ds" class="level2">
<h2 class="anchored" data-anchor-id="basic-tools-for-ai-and-ds">Basic Tools for AI and DS</h2>
<p>While the world of AI and DS is supported by a vast array of software tools and platforms, one programming language, <code>Python</code>, along with its extensive collection of specialized libraries, has emerged as a near-universal standard. <code>Python</code>’s popularity stems from its inherent readability, its versatility across a wide range of tasks, and the robust support provided by its large and active global community. We have already encountered <code>Python</code> in our earlier discussions on statistical calculations, and now we will look more closely at the libraries that make it so powerful for AI and DS.</p>
<section id="python-the-language-for-ai-ds" class="level3">
<h3 class="anchored" data-anchor-id="python-the-language-for-ai-ds"><code>Python</code>: The language for AI &amp; DS</h3>
<p><code>Python</code>’s design philosophy emphasizes code readability and a syntax that allows programmers to express concepts in fewer lines of code than might be possible in languages like C++ or Java. This makes it relatively easy for beginners to learn and for experienced programmers to quickly prototype and experiment with new ideas, which is particularly valuable in the iterative world of data analysis and model development.</p>
</section>
<section id="key-python-libraries-for-ai-and-ds" class="level3">
<h3 class="anchored" data-anchor-id="key-python-libraries-for-ai-and-ds">Key <code>Python</code> libraries for AI and DS</h3>
<p>Several external libraries significantly extend <code>Python</code>’s native capabilities, transforming it into a highly effective environment for complex data manipulation, numerical computation, machine learning, and visualization.</p>
<ul>
<li><p><strong><code>NumPy</code> (Numerical Python):</strong> The foundation for numerical computation</p>
<p>At the heart of much scientific computing in Python lies <code>NumPy</code>. It is the fundamental package for numerical computation, providing robust support for large, multi-dimensional arrays and matrices. Think of a <code>NumPy</code> array as a powerful, grid-like data structure that can hold numbers. Beyond just storing these numbers,<code>NumPy</code> offers a vast collection of high-level mathematical functions designed to operate efficiently on these arrays.</p>
<p>Key features that make <code>NumPy</code> indispensable include its <code>ndarray</code> object, which is an efficient way to store and manipulate numerical data, along with tools for common array operations like selecting specific elements (slicing and indexing), changing the shape of arrays (reshaping), performing linear algebra calculations, conducting Fourier transforms, and generating random numbers. The efficiency of <code>NumPy</code>’s operations is a critical factor when dealing with the large datasets often encountered in AI and DS. Furthermore, many other cornerstone libraries, including Pandas and <code>Scikit-learn</code>, are built directly on top of <code>NumPy</code> and utilize its <code>ndarray</code> as their primary data structure.</p>
<p>Let’s see a simple example:</p>
<div id="b2ddc115" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># We can create a NumPy array from a Python list</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>my_list <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>]</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>arr <span class="op">=</span> np.array(my_list)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"This is our NumPy array: </span><span class="sc">{</span>arr<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># NumPy allows for efficient element-wise operations</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># For instance, squaring every element in the array</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>arr_squared <span class="op">=</span> arr <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Our array with each element squared: </span><span class="sc">{</span>arr_squared<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># NumPy also handles multi-dimensional arrays, like matrices</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>matrix <span class="op">=</span> np.array([[<span class="dv">10</span>, <span class="dv">20</span>], [<span class="dv">30</span>, <span class="dv">40</span>]])</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"This is a 2D NumPy array (a matrix):</span><span class="ch">\n</span><span class="sc">{</span>matrix<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>This is our NumPy array: [1 2 3 4 5]
Our array with each element squared: [ 1  4  9 16 25]
This is a 2D NumPy array (a matrix):
[[10 20]
 [30 40]]</code></pre>
</div>
</div></li>
<li><p><strong><code>Pandas</code>:</strong> Data analysis and manipulation made easy</p>
<p>While <code>NumPy</code> provides the numerical backbone, <code>Pandas</code> offers high-performance, intuitive data structures and a rich set of tools specifically designed for practical data analysis. The two primary data structures in <code>Pandas</code> are the <code>Series</code> and the <code>DataFrame</code>. A <code>Series</code> can be thought of as a single column of data (a 1D labeled array), while a <code>DataFrame</code> is a 2D labeled data structure with columns of potentially different types, much like a spreadsheet, a SQL table, or a dictionary of Series objects.</p>
<p><code>Pandas</code> excels at tasks such as reading data from and writing data to a multitude of formats (including CSV files, Excel spreadsheets, SQL databases, and JSON). It provides powerful features for aligning data, handling missing values (a very common issue in real-world datasets), merging or joining different datasets together, reshaping data layouts, sophisticated indexing and slicing capabilities for selecting subsets of data, grouping data based on certain criteria to perform aggregate calculations, and specialized functionality for working with time series data. For data scientists and analysts, Pandas significantly simplifies the often complex and tedious processes of data cleaning, transformation, and exploration.</p>
<p>Here’s a glimpse of <code>Pandas</code> in action:</p>
<div id="abaab5c8" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a Pandas Series, which is like a labeled 1D array</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>student_scores <span class="op">=</span> pd.Series([<span class="dv">85</span>, <span class="dv">92</span>, <span class="dv">78</span>, <span class="dv">95</span>], index<span class="op">=</span>[<span class="st">'Alice'</span>, <span class="st">'Bob'</span>, <span class="st">'Charlie'</span>, <span class="st">'David'</span>], name<span class="op">=</span><span class="st">'Exam Scores'</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"A Pandas Series representing student scores:</span><span class="ch">\n</span><span class="sc">{</span>student_scores<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Score for Bob: </span><span class="sc">{</span>student_scores[<span class="st">'Bob'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a Pandas DataFrame, which is like a table</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>student_data <span class="op">=</span> {<span class="st">'StudentName'</span>: [<span class="st">'Alice'</span>, <span class="st">'Bob'</span>, <span class="st">'Charlie'</span>, <span class="st">'David'</span>],</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>                <span class="st">'Age'</span>: [<span class="dv">21</span>, <span class="dv">22</span>, <span class="dv">20</span>, <span class="dv">23</span>],</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>                <span class="st">'Major'</span>: [<span class="st">'CompSci'</span>, <span class="st">'Physics'</span>, <span class="st">'Math'</span>, <span class="st">'CompSci'</span>]}</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>students_df <span class="op">=</span> pd.DataFrame(student_data)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">A Pandas DataFrame with student information:</span><span class="ch">\n</span><span class="sc">{</span>students_df<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># We can easily access a specific column from the DataFrame</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Just the 'Major' column from our DataFrame:</span><span class="ch">\n</span><span class="sc">{</span>students_df[<span class="st">'Major'</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>A Pandas Series representing student scores:
Alice      85
Bob        92
Charlie    78
David      95
Name: Exam Scores, dtype: int64
Score for Bob: 92

A Pandas DataFrame with student information:
  StudentName  Age    Major
0       Alice   21  CompSci
1         Bob   22  Physics
2     Charlie   20     Math
3       David   23  CompSci

Just the 'Major' column from our DataFrame:
0    CompSci
1    Physics
2       Math
3    CompSci
Name: Major, dtype: object</code></pre>
</div>
</div></li>
<li><p><strong><code>Matplotlib</code> &amp; <code>Seaborn</code>:</strong> Visualizing data</p>
<p>Data visualization is an indispensable part of the DS workflow. It allows us to explore data graphically, uncover patterns, identify outliers, understand relationships between variables, and effectively communicate findings to others. <code>Matplotlib</code> is the foundational plotting library in <code>Python</code>, offering a wide range of capabilities for creating static, animated, and interactive visualizations. It can produce line plots, scatter plots, bar charts, histograms, pie charts, error charts, 3D plots, and much more, with a high degree of customization.</p>
<p><code>Seaborn</code> is another powerful visualization library that is built on top of <code>Matplotlib</code>. It provides a higher-level interface specifically designed for creating attractive and informative statistical graphics. Seaborn often makes it easier to generate common types of statistical plots like box plots (which we discussed in Unit 2), violin plots, heatmaps, distribution plots (like enhanced histograms), and plots that show relationships along with regression lines. It also comes with more aesthetically pleasing default styles.</p>
<p>Let’s illustrate with a couple of simple plots:</p></li>
</ul>
<div id="6384d386" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> numpy <span class="im">as</span> np <span class="co"># Re-importing for clarity if this cell is run standalone</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Let's generate some sample data for our plots</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    x_values <span class="op">=</span> np.linspace(<span class="op">-</span>np.pi, np.pi, <span class="dv">200</span>) <span class="co"># 200 points from -pi to pi</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    y_sine_values <span class="op">=</span> np.sin(x_values)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    y_cosine_values <span class="op">=</span> np.cos(x_values)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    some_random_data <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span><span class="dv">500</span>) <span class="co"># 500 numbers from a normal distribution</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># An example using Matplotlib to plot sine and cosine waves</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>)) <span class="co"># Set the figure size</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    plt.plot(x_values, y_sine_values, label<span class="op">=</span><span class="st">'Sine Wave'</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    plt.plot(x_values, y_cosine_values, label<span class="op">=</span><span class="st">'Cosine Wave'</span>, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Sine and Cosine Waves using Matplotlib'</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'X (radians)'</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Y (value)'</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    plt.legend() <span class="co"># Show the legend</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>) <span class="co"># Add a grid</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    plt.show() <span class="co"># Display the plot</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># An example using Seaborn to plot a histogram (distribution plot) of random data</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    sns.histplot(some_random_data, bins<span class="op">=</span><span class="dv">30</span>, kde<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">'green'</span>) <span class="co"># kde adds a density curve</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Distribution of Random Data using Seaborn'</span>)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Value'</span>)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="unit3_files/figure-html/cell-4-output-1.png" width="832" height="449" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="unit3_files/figure-html/cell-4-output-2.png" width="808" height="449" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<pre><code>The adage "a picture is worth a thousand words" is particularly true in data analysis, and these libraries are your primary tools for painting those pictures.</code></pre>
<ul>
<li><p><strong>Scikit-learn (<code>sklearn</code>):</strong> ML toolkit</p>
<p>When it comes to implementing machine learning algorithms, <code>Scikit-learn</code> (often imported as <code>sklearn</code>) is one of the most popular, comprehensive, and user-friendly libraries available in <code>Python</code>. It provides a vast array of simple and efficient tools for various data mining and data analysis tasks.</p>
<p><code>Scikit-learn</code>’s capabilities cover a wide spectrum of machine learning, including:</p>
<ul>
<li><strong>Classification:</strong> Algorithms for identifying which category an object belongs to (e.g., classifying an email as spam or not spam).</li>
<li><strong>Regression:</strong> Algorithms for predicting a continuous-valued attribute associated with an object (e.g., predicting the price of a house).</li>
<li><strong>Clustering:</strong> Algorithms for automatically grouping similar objects into sets or clusters when you don’t have pre-defined labels.</li>
<li><strong>Dimensionality Reduction:</strong> Techniques for reducing the number of variables under consideration, which can be useful for simplifying models and improving performance.</li>
<li><strong>Model Selection:</strong> Tools for comparing, validating, and choosing the best parameters and models for your specific problem.</li>
<li><strong>Preprocessing:</strong> A suite of functions for feature extraction, normalization, and other data preparation tasks necessary before feeding data to machine learning models.</li>
</ul>
<p>What makes <code>Scikit-learn</code> so valuable is its consistent and easy-to-use API (Application Programming Interface). It allows data scientists and machine learning practitioners to implement various algorithms without getting bogged down in the complex mathematical details of each algorithm’s implementation, enabling them to focus more on solving the actual problem. It is built on top of <code>NumPy</code>, <code>SciPy</code> (another scientific computing library), and <code>Matplotlib</code>. While we will delve deeper into specific <code>Scikit-learn</code> functionalities in later parts of this unit and more advanced courses, it is essential to recognize it as a core component of the AI and DS toolkit from the outset.</p>
<p>Here’s a very high-level conceptual example of how one might approach a simple predictive task using <code>Scikit-learn</code> (more detailed explanations will follow):</p></li>
</ul>
<div id="b3005d9c" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split <span class="co"># For splitting data</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression    <span class="co"># A simple regression model</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># from sklearn.metrics import mean_squared_error     # For evaluating the model</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> numpy <span class="im">as</span> np <span class="co"># Re-importing</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Let's imagine we have some very simple data:</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># X represents a single feature (e.g., years of experience)</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># y represents a target variable we want to predict (e.g., salary)</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    X_feature_sample <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>) <span class="co"># Needs to be a 2D array for sklearn</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    y_target_sample <span class="op">=</span> np.array([<span class="dv">30</span>, <span class="dv">35</span>, <span class="dv">40</span>, <span class="dv">45</span>, <span class="dv">50</span>, <span class="dv">53</span>, <span class="dv">58</span>, <span class="dv">60</span>])</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># A common practice is to split data into a training set and a testing set</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The model learns from the training set and is then evaluated on the unseen testing set</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X_feature_sample, y_target_sample, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We create an instance of a Linear Regression model</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    simple_linear_model <span class="op">=</span> LinearRegression()</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We then "train" or "fit" the model using our training data</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    simple_linear_model.fit(X_train, y_train)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Now we can use the trained model to make predictions on our test data</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    predictions_on_test_data <span class="op">=</span> simple_linear_model.predict(X_test)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"A conceptual Linear Regression Example:"</span>)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Test features (X_test):</span><span class="ch">\n</span><span class="sc">{</span>X_test<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Actual target values for test features (y_test): </span><span class="sc">{</span>y_test<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Predicted target values by the model: </span><span class="sc">{</span>predictions_on_test_data<span class="sc">.</span><span class="bu">round</span>(<span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>) <span class="co"># Rounded for readability</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We could then calculate an error metric like Mean Squared Error:</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># print(f"  Mean Squared Error on test data: {mean_squared_error(y_test, predictions_on_test_data):.2f}")</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  The model learned a slope (coefficient) of: </span><span class="sc">{</span>simple_linear_model<span class="sc">.</span>coef_[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  The model learned an intercept of: </span><span class="sc">{</span>simple_linear_model<span class="sc">.</span>intercept_<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>A conceptual Linear Regression Example:
  Test features (X_test):
[[7]
 [3]]
  Actual target values for test features (y_test): [58 40]
  Predicted target values by the model: [57.1 39.7]
  The model learned a slope (coefficient) of: 4.35
  The model learned an intercept of: 26.65</code></pre>
</div>
</div>
<p>This simple demonstration hints at the power <code>Scikit-learn</code> provides for building predictive models.</p>
<p>While these libraries – <code>NumPy</code>, <code>Pandas</code>, <code>Matplotlib</code>, <code>Seaborn</code>, and <code>Scikit-learn</code> – form the foundational toolkit for most general AI and DS tasks, the <code>Python</code> ecosystem is vast. Many other specialized libraries cater to specific advanced areas, such as deep learning (with popular frameworks like <code>TensorFlow</code>, <code>PyTorch</code>, and <code>Keras</code>), more advanced natural language processing (with libraries like <code>NLTK</code> and <code>spaCy</code>), and various other specialized analytical domains. Gaining a solid proficiency with these core libraries is the essential first step on your journey.</p>
</section>
</section>
<section id="introduction-to-ds-process-pipeline" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-ds-process-pipeline">Introduction to DS Process Pipeline</h2>
<p>Successfully tackling problems using DS is rarely a haphazard endeavor; it typically follows a structured, albeit iterative, workflow known as the DS process pipeline. While specific adaptations and names for stages might vary across organizations or projects, the underlying sequence of activities generally remains consistent. Understanding this pipeline provides a roadmap for transforming raw data into actionable insights or intelligent products.</p>
<div id="fig-pipeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="data-pipeline.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: DS Process Pipeline
</figcaption>
</figure>
</div>
<section id="business-understanding-or-problem-definition" class="level3">
<h3 class="anchored" data-anchor-id="business-understanding-or-problem-definition">Business Understanding or Problem Definition</h3>
<p>The journey usually begins with <em>business understanding or problem definition</em>. This crucial first phase involves clearly articulating the problem that needs to be solved or the question that needs an answer, from a business or domain-specific perspective. What are the overarching goals? What specific outcomes are desired? How will the success of the project be measured? Activities during this stage often include discussions with stakeholders to grasp the domain context, translating broad business challenges into well-defined DS questions, and carefully outlining the project’s scope and objectives. Without a lucid understanding of the problem, even the most sophisticated subsequent analytical efforts risk being misdirected and ultimately irrelevant.</p>
</section>
<section id="data-acquisition" class="level3">
<h3 class="anchored" data-anchor-id="data-acquisition">Data Acquisition</h3>
<p>Once the problem is well understood, the next stage is <em>Data Acquisition</em>, also referred to as data collection. The objective here is to gather all the data necessary to address the defined problem. This involves identifying potential data sources, which could range from internal company databases, external APIs (Application Programming Interfaces) that provide access to third-party data, information scraped from websites, simple flat files like CSVs or text documents, or even data streamed from sensors. After identifying sources, the data must be collected, and its format and structure must be understood. The quality and relevance of the data acquired at this stage will profoundly influence the quality of the insights derived and the performance of any models built later. The old adage “Garbage In, Garbage Out” (GIGO) is particularly pertinent here.</p>
</section>
<section id="eda" class="level3">
<h3 class="anchored" data-anchor-id="eda">EDA</h3>
<p>With data in hand, the focus shifts to <em>data understanding and exploratory data analysis (EDA)</em>. The goal of this phase is to develop a deep familiarity with the dataset’s characteristics. This involves examining its quality, identifying potential patterns, understanding relationships between different variables, and generally getting a “feel” for the data. Common activities include calculating descriptive statistics (as we learned in Unit 2, such as mean, median, standard deviation), creating various data visualizations (like histograms to see distributions, scatter plots to examine relationships between two numerical variables, and box plots to compare groups or identify outliers), systematically checking for missing values and their patterns, and spotting any unusual or extreme data points (outliers). EDA is an investigative process that helps in refining initial hypotheses about the data, guiding decisions about which features (variables) might be most important for an analysis, and informing the selection of appropriate modeling techniques for later stages. It is a critical step for uncovering preliminary insights even before any formal modeling begins.</p>
</section>
<section id="data-preparation" class="level3">
<h3 class="anchored" data-anchor-id="data-preparation">Data Preparation</h3>
<p>Following EDA, we enter what is often the most time-consuming and labor-intensive phase of the pipeline: <em>data preparation</em>. This stage, also known by terms like data pre-processing, data munging, or data wrangling, is dedicated to transforming raw, often messy, data into a clean, consistent, and suitable format for effective modeling. High-quality analytical models can only be built upon high-quality data. The specific activities in data preparation are diverse and depend heavily on the nature of the data and the intended analysis. We will delve deeper into these techniques in section 3.4, but they generally include tasks like handling missing data, correcting errors, removing duplicate entries, managing outliers, transforming data scales, and encoding data into numerical formats that algorithms can understand.</p>
</section>
<section id="modeling" class="level3">
<h3 class="anchored" data-anchor-id="modeling">Modeling</h3>
<p>Once the data is adequately prepared, the <em>modeling</em> stage begins. Here, the objective is to select, build, and train appropriate analytical or machine learning models designed to address the problem defined in the initial phase. This involves choosing algorithms suitable for the task at hand – for example, linear regression for predicting a continuous value, decision trees or logistic regression for classification tasks, or k-means for clustering data into groups. A crucial part of modeling is typically splitting the prepared data into a training set, which is used to “teach” the model, and a testing set, which is kept separate and used later to evaluate how well the model performs on unseen data. The model’s internal parameters are adjusted (or “tuned”) during the training process to best capture the patterns in the training data.</p>
</section>
<section id="evaluation" class="level3">
<h3 class="anchored" data-anchor-id="evaluation">Evaluation</h3>
<p>After a model (or several candidate models) has been trained, it must undergo rigorous <em>evaluation</em>. The purpose of this stage is to assess the model’s performance, robustness, and its ability to generalize to new, unseen data, thereby ensuring it meets the project’s objectives. Evaluation involves using appropriate metrics tailored to the type of model and problem. For instance, classification models might be evaluated using accuracy, precision, recall, or F1-score, while regression models might use Mean Squared Error (MSE) or R-squared. A key activity is testing the model on the previously set-aside test data. Techniques like cross-validation are also often employed to get a more reliable estimate of performance. Comparing different models or different versions of the same model (with different settings) is also part of this stage. Proper evaluation is vital to prevent issues like overfitting, where a model learns the training data too well, including its noise, and consequently performs poorly on new data.</p>
</section>
<section id="deployment" class="level3">
<h3 class="anchored" data-anchor-id="deployment">Deployment</h3>
<p>If a model performs satisfactorily during evaluation, it can proceed to the <em>deployment</em> phase. This is where the validated model is integrated into a production environment or an existing business process so that it can start delivering tangible value. Deployment can take many forms: it might involve creating an API that allows other software systems to send data to the model and receive its predictions, building interactive dashboards that present the model’s insights to business users, or embedding the model directly within an application.</p>
</section>
<section id="monitoring-and-maintenance" class="level3">
<h3 class="anchored" data-anchor-id="monitoring-and-maintenance">Monitoring and Maintenance</h3>
<p>Finally, the DS pipeline doesn’t truly end with deployment. The <em>monitoring and maintenance</em> stage is an ongoing process crucial for the long-term success of any deployed AI or DS solution. The objective here is to continuously monitor the model’s performance in the live environment and to update or retrain it as necessary. Over time, the statistical properties of the data being fed to the model might change (a phenomenon known as “concept drift”), or the underlying relationships the model learned might no longer hold true. Regular monitoring helps detect such degradation in performance, and periodic retraining with fresh data ensures the model remains relevant and effective.</p>
<p>It is essential to recognize that this pipeline is highly <em>iterative</em>. Data scientists frequently move back and forth between these stages. For example, insights gained during Exploratory Data Analysis might reveal significant data quality issues, necessitating a return to the Data Acquisition or Data Preparation stages. Similarly, if model evaluation shows poor performance, it might prompt a re-evaluation of the features used (leading back to Data Preparation or EDA), the choice of model, or even a refinement of the initial problem definition. This iterative nature is a hallmark of practical DS work.</p>
</section>
</section>
<section id="different-representations-of-data" class="level2">
<h2 class="anchored" data-anchor-id="different-representations-of-data">Different Representations of Data</h2>
<p>Data, the raw material of AI and DS, manifests in a variety of forms. Understanding these different representations is fundamental to selecting the appropriate methods for storage, processing, analysis, and visualization. Broadly, data can be categorized into structured, unstructured, and semi-structured types.</p>
<p><strong>Structured Data</strong> is characterized by its high degree of organization. It adheres to a pre-defined data model or schema, meaning its format and the types of data it can hold are explicitly defined beforehand. The most common representation of structured data is tabular, consisting of rows (representing individual records or observations) and columns (representing specific attributes or features of those records). Each column typically has a well-defined data type, such as integer, string, date, or boolean. This kind of data is commonly found in relational databases (managed by systems like MySQL, PostgreSQL, or Oracle) and spreadsheets (like Microsoft Excel or Google Sheets). The inherent organization of structured data makes it relatively straightforward to query, manage, and analyze using traditional data processing tools, including SQL (Structured Query Language). Examples abound in everyday business operations: customer records stored in a Customer Relationship Management (CRM) system, detailed sales transactions, or employee information managed by an Human Resources (HR) system are all typically structured data. In Python, the Pandas library, with its <code>DataFrame</code> object, provides an exceptionally powerful and convenient way to work with structured, tabular data.</p>
<p>In stark contrast, <em>Unstructured Data</em> lacks a pre-defined data model or an inherent organizational framework. It does not fit neatly into the rows and columns of traditional databases. This category encompasses a vast and rapidly growing amount of information, often in textual or multimedia formats. Examples include the content of text documents such as emails, news articles, books, and social media posts; visual information like images and photographs; audio files such as voice recordings and music; and video files. The absence of a rigid structure makes unstructured data more challenging to process and analyze using conventional methods. Specialized techniques, often drawing from fields like Natural Language Processing (<code>NLP</code>) for text, computer vision for images, and signal processing for audio, are required to extract meaningful features and insights from this type of data. In Python, specific libraries are used to handle different forms of unstructured data: <code>NLTK</code> (Natural Language Toolkit) and <code>spaCy</code> are popular for text processing; <code>Pillow</code> or <code>OpenCV</code> (Open Source Computer Vision Library) are used for image manipulation and analysis; and <code>Librosa</code> is a common choice for working with audio signals. A key step in analyzing unstructured data is often feature extraction – the process of converting the raw unstructured content into a structured format (e.g., numerical vectors) that can then be fed into analytical models.</p>
<p>Bridging the gap between these two extremes is <em>Semi-structured Data</em>. This type of data does not conform to the strict relational structure of traditional databases but possesses some organizational properties, often through the use of tags, markers, or hierarchical arrangements that separate semantic elements. It is essentially a hybrid, exhibiting some degree of structure without being as rigidly defined as fully structured data. A key characteristic of semi-structured data is that it is often self-describing; the tags or markers within the data itself provide information about its structure and meaning. Common examples include JSON (JavaScript Object Notation) files, which are widely used for data interchange on the web due to their human-readable text format and simple key-value pair structure. Another example is XML (eXtensible Markup Language) documents, which use tags to define elements and their attributes, allowing for complex hierarchical data representations. Data stored in many NoSQL databases, such as document databases like MongoDB, also often falls into the semi-structured category. Python offers built-in libraries for handling these formats, such as the <code>json</code> module for working with JSON data and the <code>xml.etree.ElementTree</code> module for parsing XML. Moreover, the versatile Pandas library can often parse JSON and XML data, converting it into its familiar DataFrame structure for easier analysis.</p>
<p>Let’s look at a simple example of how Python handles JSON, a common semi-structured format:</p>
<div id="de5984d8" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Imagine we have a string containing data in JSON format</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># This could have come from a file or an API response</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>json_data_string <span class="op">=</span> <span class="st">'''</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="st">{</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="st">    "bookTitle": "The Art of DS",</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="st">    "authors": [</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="st">        {"firstName": "Jane", "lastName": "Doe"},</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="st">        {"firstName": "John", "lastName": "Smith"}</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="st">    ],</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="st">    "publicationYear": 2023,</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="st">    "topics": ["Statistics", "Machine Learning", "Visualization"],</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="st">    "isBestseller": true</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="st">'''</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co"># We can "load" this JSON string into a Python dictionary</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co"># This makes the data easily accessible in our Python program</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>book_details_dict <span class="op">=</span> json.loads(json_data_string)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Now we can access elements of the data like a regular Python dictionary</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The title of the book is: </span><span class="sc">{</span>book_details_dict[<span class="st">'bookTitle'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The first author's last name is: </span><span class="sc">{</span>book_details_dict[<span class="st">'authors'</span>][<span class="dv">0</span>][<span class="st">'lastName'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"One of the topics covered is: </span><span class="sc">{</span>book_details_dict[<span class="st">'topics'</span>][<span class="dv">1</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Is it a bestseller? </span><span class="sc">{</span>book_details_dict[<span class="st">'isBestseller'</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>The title of the book is: The Art of DS
The first author's last name is: Doe
One of the topics covered is: Machine Learning
Is it a bestseller? True</code></pre>
</div>
</div>
<p>This example demonstrates how easily Python can parse and interact with semi-structured JSON data, making it accessible for further processing and analysis.</p>
<p>A comprehensive understanding of these different data representations—structured, unstructured, and semi-structured—is essential for any data professional. It guides the selection of appropriate storage solutions, informs the choice of data processing techniques, and dictates the analytical tools that can be effectively employed. Many real-world projects involve a blend of these data types, requiring a versatile skill set to manage and extract value from them all.</p>
<section id="importance-of-pre-processing-the-data" class="level3">
<h3 class="anchored" data-anchor-id="importance-of-pre-processing-the-data">Importance of pre-processing the data</h3>
<p>The journey from raw data to meaningful insights or effective AI models is rarely straightforward. Data collected from real-world sources – be it from databases, user interactions, sensors, or external feeds – is often far from perfect. It can be messy, riddled with inconsistencies, plagued by missing information, and generally not in a state suitable for direct input into analytical algorithms or machine learning models. This is where data pre-processing plays a pivotal role. It is a critical, and often the most time-consuming, phase in the DS pipeline. Data pre-processing encompasses a collection of techniques used to clean, transform, and organize raw data, with the ultimate goal of improving its quality and making it amenable to the subsequent stages of analysis and modeling. The quality of your input data directly dictates the quality of your output; therefore, neglecting or inadequately performing data pre-processing can lead to inaccurate models, misleading conclusions, and ultimately, a failed project. The well-known adage “Garbage In, Garbage Out” (GIGO) emphatically applies to this stage. The tasks involved in data pre-processing are diverse and depend heavily on the specific dataset and the objectives of the analysis. However, some common categories of pre-processing steps include Data Cleaning, Data Transformation, and Data Reduction.</p>
<section id="data-cleaning" class="level4">
<h4 class="anchored" data-anchor-id="data-cleaning">Data cleaning</h4>
<p>Data cleaning focuses on identifying and rectifying errors, inconsistencies, and missing information within the dataset. A very common issue is handling missing values. Often, datasets will have entries where data is absent or was not recorded. How these missing values are dealt with can significantly impact the analysis. Several strategies exist:</p>
<ul>
<li><p>One approach is deletion, which involves removing records (rows) that contain missing values, or even entire features (columns) if they have an excessive proportion of missing data and are deemed not critical. Row deletion is generally viable if only a small number of records are affected and the dataset is large enough to absorb the loss.</p></li>
<li><p>A more common approach is imputation, which involves filling in the missing values with plausible substitutes. For numerical features, missing values might be replaced with the mean or median of that feature. For categorical features, the mode (the most frequent category) is often used. More sophisticated imputation techniques also exist, such as using regression models or k-Nearest Neighbors to predict the missing values based on other information in the dataset.</p></li>
</ul>
<p>Let’s illustrate imputation with <code>Python</code> and <code>Pandas</code>:</p>
<div id="c0301a73" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np <span class="co"># For creating np.nan (Not a Number) to represent missing values</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample DataFrame with some missing values</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>raw_data <span class="op">=</span> {<span class="st">'FeatureA'</span>: [<span class="dv">10</span>, <span class="dv">20</span>, np.nan, <span class="dv">40</span>, <span class="dv">10</span>, <span class="dv">60</span>],</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">'FeatureB'</span>: [<span class="dv">100</span>, <span class="dv">120</span>, <span class="dv">110</span>, np.nan, <span class="dv">100</span>, <span class="dv">140</span>],</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Category'</span>: [<span class="st">'Alpha'</span>, <span class="st">'Beta'</span>, np.nan, <span class="st">'Alpha'</span>, <span class="st">'Gamma'</span>, <span class="st">'Beta'</span>]}</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>df_with_missing <span class="op">=</span> pd.DataFrame(raw_data)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original DataFrame with missing values:"</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_with_missing)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Strategy 1: Fill missing numerical values with the mean of their respective columns</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>df_mean_imputed <span class="op">=</span> df_with_missing.copy() <span class="co"># Work on a copy</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>df_mean_imputed[<span class="st">'FeatureA'</span>] <span class="op">=</span> df_mean_imputed[<span class="st">'FeatureA'</span>].fillna(df_mean_imputed[<span class="st">'FeatureA'</span>].mean())</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>df_mean_imputed[<span class="st">'FeatureB'</span>] <span class="op">=</span> df_mean_imputed[<span class="st">'FeatureB'</span>].fillna(df_mean_imputed[<span class="st">'FeatureB'</span>].mean())</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">DataFrame after mean imputation for FeatureA and FeatureB:"</span>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_mean_imputed)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Strategy 2: Fill missing categorical values with the mode of that column</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>df_mode_imputed <span class="op">=</span> df_with_missing.copy()</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>mode_category <span class="op">=</span> df_mode_imputed[<span class="st">'Category'</span>].mode()[<span class="dv">0</span>] <span class="co"># mode() can return multiple if ties, so take the first</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>df_mode_imputed[<span class="st">'Category'</span>] <span class="op">=</span> df_mode_imputed[<span class="st">'Category'</span>].fillna(mode_category)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">DataFrame after mode imputation for Category:"</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_mode_imputed)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Strategy 3: Drop rows that contain any missing value</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>df_rows_dropped <span class="op">=</span> df_with_missing.dropna() <span class="co"># Removes rows with any NaN</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">DataFrame after dropping rows with any missing values:"</span>)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_rows_dropped)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Original DataFrame with missing values:
   FeatureA  FeatureB Category
0      10.0     100.0    Alpha
1      20.0     120.0     Beta
2       NaN     110.0      NaN
3      40.0       NaN    Alpha
4      10.0     100.0    Gamma
5      60.0     140.0     Beta

DataFrame after mean imputation for FeatureA and FeatureB:
   FeatureA  FeatureB Category
0      10.0     100.0    Alpha
1      20.0     120.0     Beta
2      28.0     110.0      NaN
3      40.0     114.0    Alpha
4      10.0     100.0    Gamma
5      60.0     140.0     Beta

DataFrame after mode imputation for Category:
   FeatureA  FeatureB Category
0      10.0     100.0    Alpha
1      20.0     120.0     Beta
2       NaN     110.0    Alpha
3      40.0       NaN    Alpha
4      10.0     100.0    Gamma
5      60.0     140.0     Beta

DataFrame after dropping rows with any missing values:
   FeatureA  FeatureB Category
0      10.0     100.0    Alpha
1      20.0     120.0     Beta
4      10.0     100.0    Gamma
5      60.0     140.0     Beta</code></pre>
</div>
</div>
<p>The choice of imputation strategy depends on the nature of the data and the extent of missingness. Another aspect of data cleaning is handling noisy data, which includes addressing errors, correcting meaningless entries, or dealing with outliers. Outliers are data points that deviate significantly from the majority of other observations in the dataset. They can arise from measurement errors, data entry mistakes, or genuinely unusual occurrences. Strategies for dealing with noisy data include manual or programmatic correction of obvious errors (like typos in text fields). For outliers, treatment options include deletion (if they are confirmed errors or clearly unrepresentative), data transformation (e.g., applying a logarithmic transformation to a skewed feature can reduce the influence of high-value outliers), capping or Winsorizing (where extreme values are replaced by the nearest “acceptable” value, such as the 99th percentile), or binning, where numerical values are grouped into discrete intervals, which can help smooth out noise. Finally, data cleaning often involves removing duplicate records to ensure that each observation is unique and to prevent bias in the analysis.</p>
</section>
<section id="data-transformation" class="level4">
<h4 class="anchored" data-anchor-id="data-transformation">Data transformation</h4>
<p>Data transformation involves modifying the data into a more suitable format or scale for analysis and modeling. A common and important transformation is Normalization or Standardization, also known as Feature Scaling. Numerical features in a dataset often have vastly different scales and ranges (for example, a person’s age might range from 0 to 100, while their income might range from tens of thousands to millions). Many machine learning algorithms, particularly those that rely on distance calculations (like k-Nearest Neighbors or Support Vector Machines) or use gradient descent for optimization (like linear regression or neural networks), can perform poorly or converge slowly if features are on drastically different scales. Feature scaling brings all numerical features onto a comparable scale.</p>
<ul>
<li><em>Normalization</em> (Min-Max Scaling) rescales the data to a fixed range, typically between 0 and 1. The formula is <span class="math display">\[X_{\text{normalized}} = \dfrac{(X - X_{min})}{(X_{max} - X_{min})}\]</span>.</li>
<li><em>Standardization</em> (Z-score Normalization) transforms the data so that it has a mean of 0 and a standard deviation of 1. The formula is : <span class="math display">\[X_{\text{standardized}} = \dfrac{(X - mean(X))}{stdev(X)}\]</span></li>
</ul>
<p>Let’s see this in <code>Python</code> using <code>Scikit-learn</code>:</p>
<div id="b2d87e90" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> MinMaxScaler, StandardScaler</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np <span class="co"># Re-importing for clarity</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample data with varying scales</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>data_for_scaling <span class="op">=</span> np.array([[<span class="dv">1000</span>, <span class="fl">0.5</span>],</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>                             [<span class="dv">2000</span>, <span class="fl">1.0</span>],</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>                             [<span class="dv">3000</span>, <span class="fl">2.5</span>],</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>                             [<span class="dv">4000</span>, <span class="fl">5.0</span>],</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>                             [<span class="dv">10000</span>, <span class="fl">10.0</span>]], dtype<span class="op">=</span><span class="bu">float</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>df_to_scale <span class="op">=</span> pd.DataFrame(data_for_scaling, columns<span class="op">=</span>[<span class="st">'Salary'</span>, <span class="st">'ExperienceYears'</span>])</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original data for scaling:"</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_to_scale)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>min_max_scaler <span class="op">=</span> MinMaxScaler()</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>normalized_array <span class="op">=</span> min_max_scaler.fit_transform(df_to_scale)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>df_normalized <span class="op">=</span> pd.DataFrame(normalized_array, columns<span class="op">=</span>df_to_scale.columns)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Data after Min-Max Normalization (scaled to 0-1):"</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_normalized)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>standard_scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>standardized_array <span class="op">=</span> standard_scaler.fit_transform(df_to_scale)</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>df_standardized <span class="op">=</span> pd.DataFrame(standardized_array, columns<span class="op">=</span>df_to_scale.columns)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Data after Standardization (mean~0, std~1):"</span>)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_standardized)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Original data for scaling:
    Salary  ExperienceYears
0   1000.0              0.5
1   2000.0              1.0
2   3000.0              2.5
3   4000.0              5.0
4  10000.0             10.0

Data after Min-Max Normalization (scaled to 0-1):
     Salary  ExperienceYears
0  0.000000         0.000000
1  0.111111         0.052632
2  0.222222         0.210526
3  0.333333         0.473684
4  1.000000         1.000000

Data after Standardization (mean~0, std~1):
     Salary  ExperienceYears
0 -0.948683        -0.950255
1 -0.632456        -0.806277
2 -0.316228        -0.374343
3  0.000000         0.345547
4  1.897367         1.785328</code></pre>
</div>
</div>
<p>Another crucial transformation is Encoding Categorical Data. Most machine learning algorithms are designed to work with numerical input and cannot directly process categorical data (textual labels). Therefore, categorical features must be converted into a numerical representation.</p>
<ul>
<li><p><strong>Label Encoding</strong> assigns a unique integer to each distinct category (e.g., if categories are ‘Red’, ‘Green’, ‘Blue’, they might become 0, 1, 2 respectively). This is suitable for ordinal categorical data where the numerical order has meaning. However, if applied to nominal data (where categories have no inherent order), algorithms might incorrectly interpret these numbers as having an ordinal relationship (e.g., implying Blue is “greater” than Green).</p></li>
<li><p><strong>One-Hot Encoding</strong> addresses this issue for nominal data. It creates new binary (0 or 1) columns for each unique category in the original feature. For any given data record, the column corresponding to its category will have a value of 1, and all other newly created columns for that original feature will have a value of 0. This method avoids implying any ordinal relationship but can lead to a significant increase in the number of features (high dimensionality) if the original categorical feature has many unique categories.</p></li>
</ul>
<p><code>Python</code>’s <code>Pandas</code> library provides convenient functions for these encoding tasks:</p>
<div id="d1fbd220" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd <span class="co"># Re-importing for clarity</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample DataFrame with a categorical feature</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>employee_data <span class="op">=</span> {<span class="st">'EmployeeID'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>],</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>                 <span class="st">'Department'</span>: [<span class="st">'Sales'</span>, <span class="st">'HR'</span>, <span class="st">'Tech'</span>, <span class="st">'Sales'</span>, <span class="st">'HR'</span>]}</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>df_employees <span class="op">=</span> pd.DataFrame(employee_data)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Original DataFrame with a categorical 'Department' feature:"</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_employees)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Label Encoding example using Pandas factorize()</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co"># factorize returns both the integer labels and the unique categories</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>df_employees[<span class="st">'Department_LabelEncoded'</span>], department_categories <span class="op">=</span> pd.factorize(df_employees[<span class="st">'Department'</span>])</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">DataFrame with Label Encoded 'Department':"</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_employees[[<span class="st">'Department'</span>, <span class="st">'Department_LabelEncoded'</span>]])</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Unique department categories for label encoding:"</span>, department_categories)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a><span class="co"># One-Hot Encoding example using Pandas get_dummies()</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>df_one_hot_encoded <span class="op">=</span> pd.get_dummies(df_employees[<span class="st">'Department'</span>], prefix<span class="op">=</span><span class="st">'Dept'</span>)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co"># We can join this back to the original DataFrame if needed</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>df_employees_final <span class="op">=</span> pd.concat([df_employees.drop(columns<span class="op">=</span>[<span class="st">'Department_LabelEncoded'</span>]), df_one_hot_encoded], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">DataFrame after One-Hot Encoding 'Department':"</span>)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df_employees_final)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Original DataFrame with a categorical 'Department' feature:
   EmployeeID Department
0           1      Sales
1           2         HR
2           3       Tech
3           4      Sales
4           5         HR

DataFrame with Label Encoded 'Department':
  Department  Department_LabelEncoded
0      Sales                        0
1         HR                        1
2       Tech                        2
3      Sales                        0
4         HR                        1
Unique department categories for label encoding: Index(['Sales', 'HR', 'Tech'], dtype='object')

DataFrame after One-Hot Encoding 'Department':
   EmployeeID Department  Dept_HR  Dept_Sales  Dept_Tech
0           1      Sales    False        True      False
1           2         HR     True       False      False
2           3       Tech    False       False       True
3           4      Sales    False        True      False
4           5         HR     True       False      False</code></pre>
</div>
</div>
<p>Other transformations include Binning or Discretization, which involves converting continuous numerical data into a finite number of discrete bins or categories (e.g., grouping ages into “Child,” “Adolescent,” “Adult,” “Senior”). This can sometimes help manage non-linear relationships in the data or reduce the impact of minor variations or noise.</p>
</section>
<section id="data-reduction" class="level4">
<h4 class="anchored" data-anchor-id="data-reduction">Data reduction</h4>
<p>Data reduction techniques aim to reduce the volume or complexity of the data while striving to preserve the essential information it contains.</p>
<ul>
<li><p>One common approach is <em>Dimensionality Reduction</em>, which is particularly relevant when dealing with datasets that have a very large number of features (variables). High dimensionality can lead to computational inefficiency, make models harder to interpret, and increase the risk of a phenomenon known as the “curse of dimensionality” (where data becomes sparse in high-dimensional space, making it harder for algorithms to find patterns). Dimensionality reduction can be achieved through:</p></li>
<li><p><em>Feature Selection</em>: Involves selecting a subset of the most relevant original features for the analysis, discarding less important ones.</p></li>
<li><p><em>Feature Extraction</em>: Involves creating new, smaller set of features by combining or transforming the original features (e.g., Principal Component Analysis - PCA, is a popular technique for this).</p></li>
</ul>
<p>Another form of data reduction is <em>numerosity reduction</em>, which aims to reduce the number of data records (rows) while maintaining data integrity as much as possible. This can be done through various sampling techniques or by aggregating data.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Importance of data processing
</div>
</div>
<div class="callout-body-container callout-body">
<p>Effective data pre-processing is not merely a series of mechanical steps; it demands careful judgment, a good understanding of the data’s context (domain knowledge), and an awareness of how different pre-processing choices can impact the subsequent analysis and modeling stages. It is an iterative process that often requires experimentation to find the optimal preparation strategy for a given dataset and problem.</p>
</div>
</div>
</section>
</section>
</section>
<section id="elementary-applications-of-ai-and-ds" class="level2">
<h2 class="anchored" data-anchor-id="elementary-applications-of-ai-and-ds">Elementary Applications of AI and DS</h2>
<p>Having acquainted ourselves with essential tools, the structured DS process, and the critical importance of data preparation, we can now explore some elementary yet illustrative applications of AI and DS. These examples often draw upon fundamental concepts from supervised learning (where models learn from labeled data) or unsupervised learning (where models find patterns in unlabeled data), which are topics typically explored in greater depth in subsequent, more specialized courses.</p>
<section id="classification-tasks" class="level3">
<h3 class="anchored" data-anchor-id="classification-tasks">Classification tasks</h3>
<p>A common task in AI and DS is classification. The goal here is to build a model that can take an input instance (described by a set of features) and assign it to one of several pre-defined categories or classes. A widely understood example is Spam Email Detection. The problem is to automatically determine whether an incoming email is unsolicited junk mail (spam) or legitimate email (often called “ham”). To build such a system, one would typically start with a dataset of emails, where each email has already been labeled by humans as either “spam” or “ham.” The features extracted from these emails could include the presence or absence of certain keywords (e.g., “free,” “winner,” “urgent,” “money”), characteristics of the sender’s email address, the structure of the email, the number of links, and so on.</p>
<p>The approach would involve pre-processing this data, which for text often means converting the email content into a numerical format that algorithms can work with (e.g., using techniques like “bag-of-words” to count word occurrences, or more advanced methods like TF-IDF scores which reflect how important a word is to a document in a collection). Once the features are prepared, a classification algorithm (such as Naive Bayes, Logistic Regression, or a Support Vector Machine) is trained on this labeled dataset. During training, the algorithm learns the patterns and characteristics that tend to distinguish spam emails from legitimate ones. After the model is trained and evaluated, it can then be used to predict the class (spam or ham) for new, unseen emails by extracting their features and feeding them into the model. Libraries like Scikit-learn in Python provide comprehensive tools both for extracting features from text and for implementing a wide variety of classification algorithms.</p>
</section>
<section id="regression-tasks" class="level3">
<h3 class="anchored" data-anchor-id="regression-tasks">Regression tasks</h3>
<p>Another fundamental application is regression. Unlike classification, where the goal is to predict a category, regression aims to predict a continuous numerical value.</p>
<p>A classic example is house price prediction. The objective here is to predict the likely selling price of a house based on its various characteristics. The data for such a problem would typically consist of a collection of records for houses that have already been sold. Each record would include features such as the house’s size (e.g., square footage), the number of bedrooms and bathrooms, its geographical location (which might need to be encoded numerically), the age of the house, and, crucially, its actual selling price (the target variable we want to predict).</p>
<p>The process would involve pre-processing this data – for instance, handling any missing values for features, converting categorical features like location into a numerical format, and possibly scaling numerical features to ensure they are on a comparable range. Then, a regression algorithm (common choices include Linear Regression, Decision Tree Regressors, or more complex ensemble methods like Random Forest Regressors) is trained on this dataset. The model learns the underlying relationship between the house’s features and its selling price from the historical data. Once trained, this model can be used to predict the likely selling price for a new house for which we know the features but not the price. <code>Scikit-learn</code> is again a go-to library for implementing these regression models.</p>
</section>
<section id="clustering-tasks" class="level3">
<h3 class="anchored" data-anchor-id="clustering-tasks">Clustering tasks</h3>
<p>Clustering falls under the umbrella of unsupervised learning, where the goal is to find inherent groupings or structures in data without having pre-defined labels for those groups. The objective is to group a set of input instances into clusters such that instances within the same cluster are more similar to each other (based on their features) than they are to instances in other clusters.</p>
<p>A common business application is customer segmentation. The problem is to identify distinct groups of customers based on their characteristics or behaviors, such as their purchasing history (e.g., items bought, frequency of purchases, total amount spent), their activity on a company’s website, or their demographic information. The key here is that we don’t start by knowing what these segments are; we want the algorithm to discover them.</p>
<p>The approach involves selecting relevant features that describe the customers and pre-processing this data (e.g., scaling numerical features). Then, a clustering algorithm, such as K-Means or Hierarchical Clustering, is applied. The algorithm iteratively groups customers based on the similarity of their feature values. After the clusters are formed, the next step is interpretation: analyzing the characteristics of the customers within each cluster to understand what defines each segment (e.g., one cluster might represent “high-value, frequent shoppers,” another “budget-conscious, occasional buyers,” and a third “newly acquired customers”). These identified segments can then inform targeted marketing campaigns, personalized product recommendations, or tailored customer service strategies. Scikit-learn provides implementations of several widely used clustering algorithms.</p>
</section>
<section id="simple-recommendation-systems" class="level3">
<h3 class="anchored" data-anchor-id="simple-recommendation-systems">Simple recommendation systems</h3>
<p>Recommendation systems are pervasive in our digital lives, suggesting movies we might like, products we might want to buy, or news articles we might find interesting. The core goal is to predict the “rating” or “preference” a user would give to an item they have not yet considered.</p>
<p>An elementary form of recommendation can be seen in suggestions like “Users who bought product X also frequently bought product Y.” This type of recommendation often stems from analyzing co-occurrence patterns in transaction data. The problem is to suggest relevant additional products to an online shopper, perhaps while they are browsing or at the checkout stage. The data required is the purchase history of many users – specifically, information about which items were bought together in the same transactions.</p>
<p>A conceptual approach involves Association Rule Mining or basic Collaborative Filtering logic. Association rule mining (using algorithms like Apriori) aims to discover interesting relationships or associations among a set of items in a dataset. For example, it might find a rule like “If a customer buys bread and butter, they are also likely to buy milk.” Collaborative filtering works by finding users with similar tastes or items with similar appeal. Based on a user’s current shopping cart content or their past purchase history, the system can then recommend other items that are frequently associated with those items, according to the patterns learned from the broader customer base. While building sophisticated, large-scale recommendation systems is a complex field, the basic principles can be understood and even implemented for simpler cases using data manipulation tools like Pandas or specialized libraries like mlxtend for association rule mining.</p>
<p>These elementary applications—classification, regression, clustering, and simple recommendations—serve as powerful illustrations of how AI and DS techniques can be applied to extract valuable patterns from data, make informed predictions, and group information in meaningful ways to solve practical problems. They represent the foundational building blocks upon which more complex and sophisticated AI systems are constructed in advanced studies and real-world, large-scale deployments.</p>
</section>
</section>
<section id="unit-review" class="level2">
<h2 class="anchored" data-anchor-id="unit-review">Unit review</h2>
<ol type="1">
<li>Explain the primary role of the Pandas library in preparing data for an AI model. Why is its <code>DataFrame</code> structure particularly useful for representing datasets that AI algorithms, as discussed in texts like Russell &amp; Norvig, learn from?</li>
<li>Describe three distinct types of data representations (e.g., structured, unstructured, semi-structured). For each, provide a real-world example and name a Python tool or library suitable for its initial processing or interaction.</li>
<li>Outline the key stages of the Data Science process pipeline. Justify the importance of the “Data Pre-processing” stage for the successful application of AI algorithms, such as those described by Khemani.</li>
<li>Define “Feature Scaling.” Explain why it is often a critical pre-processing step before applying certain machine learning algorithms (e.g., k-Nearest Neighbors or Support Vector Machines) and name two common techniques for achieving it.</li>
<li>Distinguish clearly between a “classification” task and a “regression” task within the context of supervised learning in AI. Provide a concrete example for each, illustrating the type of output predicted.</li>
<li>What is “One-Hot Encoding”? Explain its purpose in data pre-processing and discuss a scenario where it would be preferred over simple Label Encoding for a categorical feature when preparing data for an AI model.</li>
<li>Briefly explain the concept of “clustering” as an unsupervised learning technique in AI. How might the output of a clustering algorithm be practically useful for an AI system or for deriving business insights?</li>
<li>You are given a dataset for an AI project with a numerical feature <code>income</code> (ranging from $20,000 to $500,000) and another numerical feature <code>years_of_education</code> (ranging from 8 to 20). If you were to use an AI algorithm sensitive to feature scales, what pre-processing step would you apply to these features and why?</li>
<li>Imagine you are developing an AI model to predict house prices (a regression task). During the “Evaluation” stage of the Data Science pipeline, you find your model predicts very accurately for houses similar to those in your training data but poorly for houses with slightly different characteristics. What is this issue likely called, and which stage of the pipeline would you revisit to potentially improve feature representation or model complexity?</li>
<li>How does the Scikit-learn library facilitate the practical application of “learning from examples,” a core AI paradigm detailed in textbooks by Russell &amp; Norvig and Khemani? Mention at least two distinct functionalities it provides.</li>
<li>Consider an AI agent designed to understand and summarize news articles (unstructured text data). Describe, at a high level, the challenge this data representation poses for traditional AI algorithms that typically expect structured input.</li>
<li>Briefly outline two conceptual pre-processing steps that would be necessary to transform unstructured text data (like news articles) into a format more amenable for an AI learning algorithm (e.g., a classifier to determine the topic of the article).</li>
<li>Why is the initial “Business Understanding” or “Problem Definition” phase considered so critical in the Data Science process pipeline, especially when the goal is to build a <em>useful</em> AI application?</li>
<li>Describe a scenario where handling “missing values” in a dataset would be crucial before training an AI model. Name two different methods for handling missing values and a potential downside of one of them.</li>
<li>How do visualization tools like Matplotlib and Seaborn support the “Exploratory Data Analysis (EDA)” phase of the Data Science pipeline, and why is EDA important before committing to specific AI modeling techniques?</li>
<li>Denis Rothman’s “AI by Example” often uses Python. How do such practical examples help in understanding the link between abstract AI algorithms (from theory books) and their real-world implementation and behavior?</li>
<li>What is “Dimensionality Reduction” in the context of data pre-processing? Provide one reason why a data scientist might want to reduce the dimensionality of a dataset before building an AI model.</li>
<li>Explain why the Data Science process pipeline is often described as an “iterative” process rather than a strictly linear one. Provide a specific example of why a team might need to revisit an earlier stage from a later one.</li>
<li>If an AI system, as conceptualized by Russell &amp; Norvig as a “rational agent,” needs to learn from its environment, how does the quality of the “data” (representing percepts or experiences) and its “pre-processing” impact the agent’s ability to learn effectively and act rationally?</li>
<li>Discuss the importance of the “Evaluation” stage in the Data Science pipeline. What are the risks of deploying an AI model that has not been rigorously evaluated on unseen data?</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>